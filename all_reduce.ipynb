{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6bc45a4-ae41-47c1-a4c6-f0512392f361",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mMNIST(\n\u001b[1;32m      2\u001b[0m        root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m        train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m        download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m        transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      6\u001b[0m            transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      7\u001b[0m            transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1307\u001b[39m, ), (\u001b[38;5;241m0.3081\u001b[39m, ))\n\u001b[1;32m      8\u001b[0m        ]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from math import ceil\n",
    "from random import Random\n",
    "from torch.multiprocessing import Process\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class Partition(object):\n",
    "    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "\n",
    "class DataPartitioner(object):\n",
    "    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n",
    "    # 先对index进行shuffle\n",
    "    # 然后按照size进行partition\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Network architecture. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "def partition_dataset():\n",
    "    \"\"\" Partitioning MNIST \"\"\"\n",
    "    dataset = datasets.MNIST(\n",
    "        './data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "        ]))\n",
    "    size = int(dist.get_world_size()) # 获取rank的个数\n",
    "    total_bach_size = 128\n",
    "    bsz = int(total_bach_size / float(size)) # 每个rank对应的batch size\n",
    "    partition_sizes = [1.0 / size for _ in range(size)] # 设置每个rank处理数据量的大小\n",
    "    partition = DataPartitioner(dataset, partition_sizes) # 数据切分\n",
    "    partition = partition.use(dist.get_rank()) # 获取当前rank对应的数据\n",
    "\n",
    "    train_set = torch.utils.data.DataLoader(partition, batch_size=bsz, shuffle=True)\n",
    "    return train_set, bsz\n",
    "\n",
    "\n",
    "def average_gradients(model):\n",
    "    \"\"\" Gradient averaging. \"\"\"\n",
    "    size = float(dist.get_world_size())\n",
    "    for param in model.parameters():\n",
    "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n",
    "        param.grad.data /= size\n",
    "\n",
    "\n",
    "def run(rank, size):\n",
    "    \"\"\" Distributed Synchronous SGD Example \"\"\"\n",
    "    torch.manual_seed(1234)\n",
    "    train_set, bsz = partition_dataset()\n",
    "    model = Net()\n",
    "    model = model\n",
    "    model = model.cuda(rank)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    num_batches = ceil(len(train_set.dataset) / float(bsz))\n",
    "    for epoch in range(10):\n",
    "        epoch_loss = 0.0\n",
    "        for data, target in train_set:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            average_gradients(model)\n",
    "            optimizer.step()\n",
    "        print('Rank ',\n",
    "              dist.get_rank(), ', epoch ', epoch, ': ',\n",
    "              epoch_loss / num_batches)\n",
    "\n",
    "\n",
    "def init_processes(rank, size, fn, backend='nccl'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09174a8-5409-400d-8d11-812d4feb389d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
